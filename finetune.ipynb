{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOiAmyGZwlGtow9TJuZOwpI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BharathChinthallapalli/prompttune/blob/main/finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "global-configs-markdown"
      },
      "source": [
        "## Global Configurations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Global Configurations ---\n",
        "from peft import PromptTuningInit # Added here for clarity\n",
        "import os\n",
        "import random\n",
        "import torch # For device check in interactive cell\n",
        "import io # For reading uploaded files\n",
        "\n",
        "# Model Configuration\n",
        "base_model_name = \"bigscience/bloomz-560m\"  # Base model for fine-tuning\n",
        "\n",
        "# PEFT Configuration\n",
        "peft_num_virtual_tokens = 8\n",
        "peft_prompt_tuning_init = PromptTuningInit.RANDOM\n",
        "\n",
        "# Tokenizer Configuration\n",
        "max_seq_length = 128  # Maximum sequence length for tokenizer\n",
        "\n",
        "# Training Configuration\n",
        "training_output_dir = \"./prompt_tuned_model\" # Used for PEFT adapter too\n",
        "training_learning_rate = 5e-4\n",
        "training_num_epochs = 2 # Keep low for quick demo; increase for better results\n",
        "training_per_device_batch_size = 2\n",
        "training_report_to = \"none\" # Set to \"wandb\" or \"tensorboard\" if needed\n",
        "\n",
        "# Evaluation Configuration\n",
        "evaluation_per_device_batch_size = 2\n",
        "evaluation_limit_samples = 20 # Number of validation samples to evaluate on, set to None for all\n",
        "\n",
        "print(\"Global configurations set.\")"
      ],
      "metadata": {
        "id": "global-configs-code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries and set environment variables.\n",
        "!pip install --quiet \"transformers>=4.38.0\" \"peft>=0.8.0\" \"datasets\" \"accelerate\" \"bert-score\" \"evaluate\" \"fsspec>=2023.5.0\"\n",
        "\n",
        "import os\n",
        "# Set TOKENIZERS_PARALLELISM to false to avoid potential deadlocks with tokenizers when using fork.\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "\n",
        "import transformers\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "print(\"Pip install cell complete.\")"
      ],
      "metadata": {
        "id": "evgtNnQinEYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for file handling and data manipulation.\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "# import io # Moved to global config cell\n",
        "\n",
        "# Prompt user to upload CSV files.\n",
        "print(\"Upload ALL your .csv files (train/val/prompt/response/etc).\")\n",
        "uploads = files.upload()\n",
        "print(f\"Uploaded file keys: {list(uploads.keys())}\") \n",
        "\n",
        "csv_files = [fname for fname in uploads.keys() if fname.endswith('.csv')]\n",
        "print(f\"Detected CSV files: {csv_files}\") \n",
        "\n",
        "dfs = {}\n",
        "for fname in csv_files:\n",
        "    print(f\"Processing {fname}...\") \n",
        "    dfs[fname] = pd.read_csv(io.BytesIO(uploads[fname]))\n",
        "\n",
        "print(f\"Loaded {len(dfs)} dataframes: {list(dfs.keys())}\") \n",
        "print(\"Columns for each loaded dataframe:\")\n",
        "for fname, df in dfs.items():\n",
        "    print(f\"{fname}: {list(df.columns)}\")"
      ],
      "metadata": {
        "id": "5SEoUqW5nE2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper: Find best-matching column for a role\n",
        "def auto_col(df, choices):\n",
        "    \"\"\"Automatically selects the best column name from a list of choices.\n",
        "    Tries to find an exact match first, then a case-insensitive match.\n",
        "    :param df: The DataFrame to search for columns. :type df: pandas.DataFrame\n",
        "    :param choices: A list of column names to search for, in order of preference. :type choices: list[str]\n",
        "    :return: The best matching column name, or None if no match is found. :rtype: str | None\n",
        "    \"\"\"\n",
        "    for c in choices:\n",
        "        if c in df.columns: return c\n",
        "    for c in choices:\n",
        "        for cc in df.columns:\n",
        "            if cc.lower() == c.lower(): return cc\n",
        "    return None\n",
        "\n",
        "all_train = []\n",
        "all_val = []\n",
        "first_sft_processed = False \n",
        "\n",
        "for fname, df in dfs.items():\n",
        "    print(f\"Extracting SFT pairs from {fname}...\") \n",
        "    if 'improved_instruction' in df.columns:\n",
        "        orig = auto_col(df, ['original_prompt', 'prompt', 'input'])\n",
        "        ctx  = auto_col(df, ['context', 'task_context', ''])\n",
        "        instr = auto_col(df, ['instruction'])\n",
        "        tgt = auto_col(df, ['improved_instruction', 'target'])\n",
        "        for _, row in df.iterrows():\n",
        "            input_str = f\"Original Prompt: {str(row.get(orig,''))}\"\n",
        "            if ctx and str(row.get(ctx,'')) and str(row.get(ctx,''))!='nan':\n",
        "                input_str += f\"\\nContext: {row[ctx]}\"\n",
        "            if instr and str(row.get(instr,'')) and str(row.get(instr,'')) != str(row.get(orig,'')):\n",
        "                input_str += f\"\\nInstruction: {row[instr]}\"\n",
        "            output_str = str(row[tgt])\n",
        "            all_train.append({'input':input_str.strip(), 'output':output_str.strip()})\n",
        "            if not first_sft_processed:\n",
        "                print(f\"  Sample input_str for SFT: {input_str.strip()}\") \n",
        "                print(f\"  Sample output_str for SFT: {output_str.strip()}\") \n",
        "                first_sft_processed = True\n",
        "    if 'bad_prompt' in df.columns and 'good_prompt' in df.columns:\n",
        "        tdesc = auto_col(df, ['task_description'])\n",
        "        tech = auto_col(df, ['prompting_techniques'])\n",
        "        for _, row in df.iterrows():\n",
        "            input_ = f\"Task: {row[tdesc]}\\nBad Prompt: {row['bad_prompt']}\\nTechniques: {row[tech]}\"\n",
        "            all_train.append({'input': input_, 'output': row['good_prompt']})\n",
        "    if 'Base_Prompt' in df.columns and 'V1_Prompt' in df.columns and 'V2_instruction' in df.columns:\n",
        "        for _, row in df.iterrows():\n",
        "            if str(row['Base_Prompt']) and str(row['V1_Prompt']):\n",
        "                all_train.append({'input': row['Base_Prompt'], 'output': row['V1_Prompt']})\n",
        "            if str(row['V1_Prompt']) and str(row['V2_instruction']):\n",
        "                all_train.append({'input': row['V1_Prompt'], 'output': row['V2_instruction']})\n",
        "print(f\"Total SFT pairs initially extracted: {len(all_train)}\") \n",
        "\n",
        "# import random # Moved to global config cell\n",
        "random.shuffle(all_train)\n",
        "print(\"Shuffled all_train list.\") \n",
        "\n",
        "split = int(0.9*len(all_train))\n",
        "train_data = all_train[:split]\n",
        "val_data = all_train[split:]\n",
        "\n",
        "print(f\"Number of training samples: {len(train_data)}\") \n",
        "print(f\"Number of validation samples: {len(val_data)}\") \n",
        "print(f\"Example training sample after processing: {train_data[0] if train_data else 'N/A'}\") "
      ],
      "metadata": {
        "id": "ZxOp5u2bnHua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_dataset = Dataset.from_pandas(pd.DataFrame(train_data))\n",
        "val_dataset = Dataset.from_pandas(pd.DataFrame(val_data))\n",
        "\n",
        "print(f\"Hugging Face Train Dataset: {train_dataset}\") \n",
        "print(f\"Hugging Face Validation Dataset: {val_dataset}\") "
      ],
      "metadata": {
        "id": "YhE39e_1nSH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# model_name is now base_model_name from global config\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name) # Use global config\n",
        "print(f\"Tokenizer loaded for {base_model_name}.\") \n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "print(f\"Model {base_model_name} loaded.\") \n",
        "\n",
        "def preprocess(batch):\n",
        "    \"\"\"Tokenizes the input and output batches for model training.\n",
        "    :param batch: A batch of data. :type batch: dict\n",
        "    :return: Tokenized inputs with labels. :rtype: dict\n",
        "    \"\"\"\n",
        "    if batch['input']:\n",
        "        print(f\"Original input to preprocess (first item): {batch['input'][0]}\")\n",
        "    if batch['output']:\n",
        "        print(f\"Original output to preprocess (first item): {batch['output'][0]}\")\n",
        "    \n",
        "    inputs = tokenizer(\n",
        "        batch['input'], truncation=True, padding='max_length', max_length=max_seq_length # Use global config\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        batch['output'], truncation=True, padding='max_length', max_length=max_seq_length # Use global config\n",
        "    )\n",
        "    inputs['labels'] = labels['input_ids']\n",
        "    return inputs\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess, batched=True)\n",
        "val_dataset = val_dataset.map(preprocess, batched=True)\n",
        "print(f\"Train dataset after preprocessing: {train_dataset}\") \n",
        "if len(train_dataset) > 0:\n",
        "    print(f\"Sample processed train item: {train_dataset[0]}\") \n",
        "\n",
        "train_dataset.set_format(type='torch', columns=['input_ids','attention_mask','labels'])\n",
        "val_dataset.set_format(type='torch', columns=['input_ids','attention_mask','labels'])"
      ],
      "metadata": {
        "id": "OiLjrU38nWNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model, PromptTuningConfig, TaskType # PromptTuningInit is in global_config cell\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "tuning_config = PromptTuningConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    prompt_tuning_init=peft_prompt_tuning_init, # Use global config\n",
        "    num_virtual_tokens=peft_num_virtual_tokens, # Use global config\n",
        "    tokenizer_name_or_path=base_model_name # Use global config\n",
        ")\n",
        "peft_model = get_peft_model(model, tuning_config)\n",
        "print(f\"PEFT model created with {tuning_config.num_virtual_tokens} virtual tokens.\") \n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=training_output_dir, # Use global config\n",
        "    per_device_train_batch_size=training_per_device_batch_size, # Use global config\n",
        "    per_device_eval_batch_size=evaluation_per_device_batch_size, # Use global config\n",
        "    learning_rate=training_learning_rate, # Use global config\n",
        "    num_train_epochs=training_num_epochs, # Use global config\n",
        "    logging_steps=10,\n",
        "    report_to=training_report_to # Use global config\n",
        ")\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "print(\"Starting model training...\") \n",
        "trainer.train()\n",
        "print(\"Model training complete.\") "
      ],
      "metadata": {
        "id": "YMMOSlsbogHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Save PEFT Adapter and Tokenizer ---\n",
        "print(f\"Saving PEFT adapter and tokenizer to {training_output_dir}/final_adapter...\")\n",
        "peft_model_path = f\"{training_output_dir}/final_adapter\"\n",
        "trainer.model.save_pretrained(peft_model_path)\n",
        "tokenizer.save_pretrained(peft_model_path) # Save tokenizer with the adapter\n",
        "print(f\"PEFT adapter and tokenizer saved to {peft_model_path}\")\n",
        "\n",
        "# Optional: Persist to Colab disk if needed for later sessions (requires mounting Google Drive)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !mkdir -p /content/drive/My\\ Drive/prompt_tuned_model_adapters/\n",
        "# !cp -r {peft_model_path} /content/drive/My\\ Drive/prompt_tuned_model_adapters/\n",
        "# print(f\"Adapter also copied to Google Drive: /content/drive/My Drive/prompt_tuned_model_adapters/{os.path.basename(peft_model_path)}\")"
      ],
      "metadata": {"id": "save-adapter-cell"},
      "execution_count": null,
      "outputs": []
    },
    {
        "cell_type": "markdown",
        "metadata": {"id": "load-adapter-markdown"},
        "source": [
            "## Load Fine-tuned PEFT Adapter and Run Inference"
        ]
    },
    {
        "cell_type": "code",
        "source": [
            "# --- Load Fine-tuned PEFT Adapter ---\n",
            "from transformers import AutoModelForCausalLM, AutoTokenizer # Ensure these are imported\n",
            "from peft import PeftModel # Ensure PeftModel is imported\n",
            "# import random # Already in global config cell\n",
            "# import os # Already in global config cell\n",
            "\n",
            "print(f\"Loading base model ({base_model_name}) for PEFT adapter...\")\n",
            "\n",
            "base_model_for_loading = AutoModelForCausalLM.from_pretrained(base_model_name, trust_remote_code=True)\n",
            "tokenizer_for_loading = AutoTokenizer.from_pretrained(base_model_name) # Use base_model_name for consistency\n",
            "\n",
            "peft_adapter_path = f\"{training_output_dir}/final_adapter\"\n",
            "print(f\"Loading PEFT adapter from: {peft_adapter_path}\")\n",
            "\n",
            "loaded_peft_model = PeftModel.from_pretrained(base_model_for_loading, peft_adapter_path)\n",
            "loaded_peft_model.to(base_model_for_loading.device) \n",
            "loaded_peft_model.eval() \n",
            "\n",
            "print(\"PEFT model with fine-tuned adapter loaded successfully.\")\n",
            "\n",
            "# --- Example Inference with Loaded Adapter ---\n",
            "if 'train_data' in globals() and train_data: \n",
            "    fewshot_examples_for_loaded = random.sample(train_data, 2) \n",
            "    test_prompt_loaded = \"Describe a futuristic city.\"\n",
            "    \n",
            "    # build_fewshot_prompt should be defined in a previous cell\n",
            "    fewshot_input_loaded_str = build_fewshot_prompt(test_prompt_loaded, fewshot_examples=fewshot_examples_for_loaded)\n",
            "    print(f\"\\nTest prompt for loaded model: {test_prompt_loaded}\")\n",
            "    print(f\"Few-shot input string for loaded model:\\n{fewshot_input_loaded_str}\")\n",
            "\n",
            "    inputs_loaded = tokenizer_for_loading(fewshot_input_loaded_str, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_seq_length)\n",
            "    inputs_loaded = {k: v.to(loaded_peft_model.device) for k, v in inputs_loaded.items()}\n",
            "\n",
            "    print(\"\\nGenerating output with loaded model...\")\n",
            "    outputs_loaded = loaded_peft_model.generate(\n",
            "        input_ids=inputs_loaded[\"input_ids\"],\n",
            "        attention_mask=inputs_loaded[\"attention_mask\"],\n",
            "        max_new_tokens=100, \n",
            "        eos_token_id=tokenizer_for_loading.eos_token_id,\n",
            "        repetition_penalty=1.2\n",
            "    )\n",
            "    decoded_output_loaded = tokenizer_for_loading.decode(outputs_loaded[0], skip_special_tokens=True)\n",
            "    \n",
            "    answer_start_index = decoded_output_loaded.rfind(\"A:\") + 2\n",
            "    final_answer_loaded = decoded_output_loaded[answer_start_index:].strip() if answer_start_index > 1 else decoded_output_loaded\n",
            "\n",
            "    print(f\"\\nGenerated Answer (loaded model):\\n{final_answer_loaded}\")\n",
            "else:\n",
            "    print(\"Skipping inference with loaded model as train_data is not available to create few-shot examples.\")"
        ],
        "metadata": {"id": "load-adapter-code"},
        "execution_count": null,
        "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_fewshot_prompt(user_prompt, fewshot_examples=[]):\n",
        "    \"\"\"Builds a few-shot prompt string from examples and a user query.\n",
        "    :param user_prompt: The user's query. :type user_prompt: str\n",
        "    :param fewshot_examples: List of dicts, each with 'input'/'output'. :type fewshot_examples: list[dict]\n",
        "    :return: The constructed few-shot prompt. :rtype: str\n",
        "    \"\"\"\n",
        "    print(f\"Building few-shot prompt for user_prompt: {user_prompt[:100]}...\")\n",
        "    s = \"\"\n",
        "    for ex in fewshot_examples:\n",
        "        s += f\"Q: {ex['input']}\\nA: {ex['output']}\\n\"\n",
        "    s += f\"Q: {user_prompt}\\nA:\"\n",
        "    print(f\"Constructed few-shot prompt (first 200 chars): {s[:200]}...\")\n",
        "    return s\n",
        "\n",
        "# This cell is for testing the build_fewshot_prompt function and inference with the original peft_model.\n",
        "# Ensure 'train_data' is available from data preparation steps.\n",
        "if 'train_data' in globals() and train_data:\n",
        "    fewshot_examples_test = random.sample(train_data, 2) # Re-sample or use existing 'fewshot_examples'\n",
        "    test_prompt_build = \"Make me a summary about Berlin nightlife\"\n",
        "    print(f\"\\nTesting build_fewshot_prompt with: '{test_prompt_build}'\")\n",
        "    fewshot_input_str = build_fewshot_prompt(test_prompt_build, fewshot_examples=fewshot_examples_test)\n",
        "\n",
        "    inputs_test = tokenizer(fewshot_input_str, return_tensors=\"pt\")\n",
        "    inputs_test = {k: v.to(peft_model.device) for k, v in inputs_test.items()}\n",
        "\n",
        "    print(\"\\nGenerating output with original peft_model for test_prompt_build...\")\n",
        "    outputs_test = peft_model.generate(\n",
        "        input_ids=inputs_test[\"input_ids\"], attention_mask=inputs_test[\"attention_mask\"],\n",
        "        max_new_tokens=128,\n",
        "        repetition_penalty=1.2,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    decoded_output_test = tokenizer.decode(outputs_test[0], skip_special_tokens=True)\n",
        "    print(f\"Decoded output (original peft_model):\n{decoded_output_test}\")\n",
        "else:\n",
        "    print(\"Skipping build_fewshot_prompt test as train_data is not available.\")"
      ],
      "metadata": {
        "id": "ZO_Aaq9Jo0WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "# rouge = evaluate.load(\"rouge\") # Defined in global config cell\n",
        "# bertscore = evaluate.load(\"bertscore\") # Defined in global config cell\n",
        "\n",
        "def eval_on_val(model_to_eval, tokenizer_to_use, val_data_subset, current_fewshot_examples):\n",
        "    \"\"\"Evaluates the model on the validation set using ROUGE and BERTScore.\n",
        "    :param model_to_eval: The model to evaluate (e.g. peft_model or loaded_peft_model)\n",
        "    :type model_to_eval: PeftModel | AutoModelForCausalLM\n",
        "    :param tokenizer_to_use: The tokenizer for the model.\n",
        "    :type tokenizer_to_use: AutoTokenizer\n",
        "    :param val_data_subset: The validation data subset.\n",
        "    :type val_data_subset: list[dict]\n",
        "    :param current_fewshot_examples: Few-shot examples to use in prompt construction.\n",
        "    :type current_fewshot_examples: list[dict]\n",
        "    \"\"\"\n",
        "    print(\"Starting evaluation on validation set...\")\n",
        "    refs, preds = [], []\n",
        "    # Use evaluation_limit_samples from global_config\n",
        "    eval_samples = val_data_subset[:evaluation_limit_samples] if evaluation_limit_samples is not None else val_data_subset\n",
        "\n",
        "    for idx, item in enumerate(eval_samples):\n",
        "        if idx < 3: # Print details for the first 3 samples\n",
        "            print(f\"  Evaluating item {idx+1} - Input: {item['input'][:100]}...\")\n",
        "        \n",
        "        fewshot_input_str = build_fewshot_prompt(item['input'], fewshot_examples=current_fewshot_examples)\n",
        "        inp = tokenizer_to_use(fewshot_input_str, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_seq_length) # Use global max_seq_length\n",
        "        inp = {k: v.to(model_to_eval.device) for k, v in inp.items()}\n",
        "        \n",
        "        out = model_to_eval.generate(\n",
        "            input_ids=inp[\"input_ids\"], attention_mask=inp[\"attention_mask\"],\n",
        "            max_new_tokens=max_seq_length, # Max new tokens can also be parameterized\n",
        "            eos_token_id=tokenizer_to_use.eos_token_id,\n",
        "            repetition_penalty=1.2 # Added from previous inference example\n",
        "        )\n",
        "        \n",
        "        input_length = inp[\"input_ids\"].shape[1]\n",
        "        generated_tokens = out[0][input_length:]\n",
        "        pred = tokenizer_to_use.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "        if idx < 3:\n",
        "            print(f\"    Generated prediction for item {idx+1}: {pred[:100]}...\")\n",
        "        \n",
        "        preds.append(pred)\n",
        "        refs.append(item['output'].strip())\n",
        "    \n",
        "    # Load metrics if not already loaded (e.g. if cell is run independently)\n",
        "    rouge_metric = evaluate.load(\"rouge\")\n",
        "    bertscore_metric = evaluate.load(\"bertscore\")\n",
        "\n",
        "    results_rouge = rouge_metric.compute(predictions=preds, references=refs)\n",
        "    results_bertscore = bertscore_metric.compute(predictions=preds, references=refs, lang=\"en\")\n",
        "    \n",
        "    print(f\"Evaluation - ROUGE-L: {results_rouge['rougeL']}\")\n",
        "    avg_bertscore_f1 = sum(results_bertscore['f1']) / len(results_bertscore['f1']) if results_bertscore['f1'] else 0\n",
        "    print(f\"Evaluation - BERTScore F1 (avg): {avg_bertscore_f1}\")\n",
        "\n",
        "# Ensure fewshot_examples is defined (e.g., from cell 10's logic or re-run here)\n",
        "if 'train_data' in globals() and train_data:\n",
        "    if 'fewshot_examples' not in globals(): # If not defined by cell 10\n",
        "        fewshot_examples = random.sample(train_data, 2)\n",
        "    eval_on_val(peft_model, tokenizer, val_data, fewshot_examples)\n",
        "else:\n",
        "    print(\"Skipping eval_on_val as train_data for fewshot_examples is not available.\")"
      ],
      "metadata": {
        "id": "ggD9zSb_tf__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_analyze_flaws(prompt, model, tokenizer, max_tokens=128):\n",
        "    \"\"\"Analyzes a given prompt for flaws using the LLM.\n",
        "    :param prompt: The user prompt to analyze. :type prompt: str\n",
        "    :param model: The language model. :type model: PeftModel | AutoModelForCausalLM\n",
        "    :param tokenizer: The tokenizer. :type tokenizer: AutoTokenizer\n",
        "    :param max_tokens: Max new tokens for analysis. :type max_tokens: int\n",
        "    :return: Analysis of flaws. :rtype: str\n",
        "    \"\"\"\n",
        "    print(f\"llm_analyze_flaws - Input prompt (first 100 chars): {prompt[:100]}...\")\n",
        "    query = (\n",
        "        f\"Analyze the following user prompt for weaknesses or areas for improvement. \"\n",
        "        f\"Be specific (e.g., 'vague', 'missing role', 'no output format', 'ambiguous', etc.).\\n\"\n",
        "        f\"Prompt:\\n{prompt}\\nList the flaws as bullet points.\"\n",
        "    )\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=max_tokens, eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    input_length = inputs[\"input_ids\"].shape[1]\n",
        "    generated_tokens = outputs[0][input_length:]\n",
        "    flaws_analysis = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "    print(f\"llm_analyze_flaws - LLM Analysis Result: {flaws_analysis}\")\n",
        "    return flaws_analysis"
      ],
      "metadata": {
        "id": "rTjUBjjlyqrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_recommend_techniques(prompt, flaws, model, tokenizer, max_tokens=128):\n",
        "    \"\"\"Recommends prompt engineering techniques.\n",
        "    :param prompt: Original user prompt. :type prompt: str\n",
        "    :param flaws: Detected flaws. :type flaws: str\n",
        "    :param model: Language model. :type model: PeftModel | AutoModelForCausalLM\n",
        "    :param tokenizer: Tokenizer. :type tokenizer: AutoTokenizer\n",
        "    :param max_tokens: Max new tokens for recommendations. :type max_tokens: int\n",
        "    :return: Recommended techniques. :rtype: str\n",
        "    \"\"\"\n",
        "    print(f\"llm_recommend_techniques - Input prompt (first 100 chars): {prompt[:100]}...\")\n",
        "    print(f\"llm_recommend_techniques - Detected flaws: {flaws}\")\n",
        "    query = (\n",
        "        f\"Given this user prompt:\\n{prompt}\\n\"\n",
        "        f\"And these detected flaws:\\n{flaws}\\n\"\n",
        "        f\"List 2-4 specific prompt engineering techniques (e.g., 'CHAIN_OF_THOUGHT', 'SPECIFY_OUTPUT_FORMAT', \"\n",
        "        f\"'ROLE_PROMPTING', 'ADD_EXAMPLES', 'ADD_CONSTRAINTS', etc.) that would improve the prompt. \"\n",
        "        f\"List only technique names as a bullet list.\"\n",
        "    )\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=max_tokens, eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    input_length = inputs[\"input_ids\"].shape[1]\n",
        "    generated_tokens = outputs[0][input_length:]\n",
        "    techniques_recommendation = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "    print(f\"llm_recommend_techniques - LLM Recommended Techniques: {techniques_recommendation}\")\n",
        "    return techniques_recommendation"
      ],
      "metadata": {
        "id": "UjtPgt25zMu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_synthesize_prompt(prompt, flaws, techniques, model, tokenizer, max_tokens=128):\n",
        "    \"\"\"Synthesizes an improved prompt using LLM.\n",
        "    :param prompt: Original user prompt. :type prompt: str\n",
        "    :param flaws: Detected flaws. :type flaws: str\n",
        "    :param techniques: Recommended techniques. :type techniques: str\n",
        "    :param model: Language model. :type model: PeftModel | AutoModelForCausalLM\n",
        "    :param tokenizer: Tokenizer. :type tokenizer: AutoTokenizer\n",
        "    :param max_tokens: Max new tokens for the synthesized prompt. :type max_tokens: int\n",
        "    :return: Improved prompt. :rtype: str\n",
        "    \"\"\"\n",
        "    print(f\"llm_synthesize_prompt - Input prompt (first 100 chars): {prompt[:100]}...\")\n",
        "    print(f\"llm_synthesize_prompt - Flaws: {flaws}\")\n",
        "    print(f\"llm_synthesize_prompt - Techniques: {techniques}\")\n",
        "    query = (\n",
        "        f\"You are an expert prompt engineer. \"\n",
        "        f\"Improve the following user prompt by explicitly addressing the listed flaws and applying these techniques.\\n\"\n",
        "        f\"User prompt: {prompt}\\n\"\n",
        "        f\"Detected flaws:\\n{flaws}\\n\"\n",
        "        f\"Techniques to use:\\n{techniques}\\n\"\n",
        "        f\"Write an improved prompt.\"\n",
        "    )\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=max_tokens, eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    input_length = inputs[\"input_ids\"].shape[1]\n",
        "    generated_tokens = outputs[0][input_length:]\n",
        "    improved_prompt_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "    print(f\"llm_synthesize_prompt - LLM Synthesized Prompt: {improved_prompt_text}\")\n",
        "    return improved_prompt_text"
      ],
      "metadata": {
        "id": "2JD6F1r7zOql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_prompt_chain(user_prompt, chain_model, chain_tokenizer, verbose=True):\n",
        "    \"\"\"Runs a 3-step LLM chain to analyze and improve a prompt.\n",
        "    :param user_prompt: The user prompt to improve. :type user_prompt: str\n",
        "    :param chain_model: The model to use for the chain. :type chain_model: PeftModel | AutoModelForCausalLM\n",
        "    :param chain_tokenizer: The tokenizer for the model. :type chain_tokenizer: AutoTokenizer\n",
        "    :param verbose: Whether to print intermediate steps. :type verbose: bool\n",
        "    :return: The improved prompt. :rtype: str\n",
        "    \"\"\"\n",
        "    print(f\"Executing LLM Prompt Chain for: {user_prompt}\")\n",
        "\n",
        "    print(\"Step 1: Analyzing flaws...\")\n",
        "    flaws = llm_analyze_flaws(user_prompt, chain_model, chain_tokenizer)\n",
        "    if verbose:\n",
        "        print(\"\\nDetected Flaws:\\n\", flaws, \"\\n\", \"-\"*40)\n",
        "\n",
        "    print(\"Step 2: Recommending techniques...\")\n",
        "    techniques = llm_recommend_techniques(user_prompt, flaws, chain_model, chain_tokenizer)\n",
        "    if verbose:\n",
        "        print(\"\\nRecommended Techniques:\\n\", techniques, \"\\n\", \"-\"*40)\n",
        "\n",
        "    print(\"Step 3: Synthesizing improved prompt...\")\n",
        "    improved = llm_synthesize_prompt(user_prompt, flaws, techniques, chain_model, chain_tokenizer)\n",
        "    if verbose:\n",
        "        print(\"\\nImproved Prompt:\\n\", improved, \"\\n\", \"-\"*60)\n",
        "    return improved\n",
        "\n",
        "# Example of using the chain with the trained peft_model\n",
        "# Ensure the model is on the correct device before calling the chain\n",
        "if 'peft_model' in globals() and 'tokenizer' in globals():\n",
        "    print(\"\\n--- Testing LLM Prompt Improvement Chain with peft_model ---\")\n",
        "    # Ensure model is on the correct device (e.g., 'cuda' if available, else 'cpu')\n",
        "    target_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if next(peft_model.parameters()).device.type != target_device:\n",
        "         print(f\"Moving peft_model to {target_device} for LLM chain test.\")\n",
        "         peft_model.to(target_device)\n",
        "    \n",
        "    test_chain_prompt = \"Make me a summary about Berlin nightlife\"\n",
        "    improved_test_prompt = llm_prompt_chain(test_chain_prompt, peft_model, tokenizer, verbose=True)\n",
        "    print(\"\\n--- End of LLM Chain Test ---\")\n",
        "    print(f\"Original Test Prompt: {test_chain_prompt}\")\n",
        "    print(f\"Chain's Improved Test Prompt: {improved_test_prompt}\")\n",
        "else:\n",
        "    print(\"Skipping LLM Prompt Chain test as peft_model or tokenizer is not available.\")"
      ],
      "metadata": {
        "id": "E4ldjV_IzQzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
        "cell_type": "markdown",
        "metadata": {"id": "interactive-prompt-markdown"},
        "source": [
            "## Interactive Prompt Improvement"
        ]
    },
    {
        "cell_type": "code",
        "source": [
            "# --- Interactive Prompt Improvement Cell ---\n",
            "# import torch # Already imported in global config cell\n",
            "\n",
            "print(\"Ensure 'peft_model' (from training) or 'loaded_peft_model' (if loaded) and 'tokenizer' are available.\")\n",
            "\n",
            "# Determine which model to use (prefer loaded, fallback to trained)\n",
            "interactive_model = None\n",
            "if 'loaded_peft_model' in globals():\n",
            "    interactive_model = loaded_peft_model\n",
            "    print(\"Using 'loaded_peft_model' for interactive session.\")\n",
            "elif 'peft_model' in globals():\n",
            "    interactive_model = peft_model\n",
            "    print(\"Using 'peft_model' from training for interactive session.\")\n",
            "else:\n",
            "    print(\"Error: No suitable model (peft_model or loaded_peft_model) found for interactive session.\")\n",
            "\n",
            "if interactive_model and 'tokenizer' in globals():\n",
            "    # Ensure model is on the correct device\n",
            "    target_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
            "    if next(interactive_model.parameters()).device.type != target_device:\n",
            "        print(f\"Moving interactive_model to {target_device}.\")\n",
            "        interactive_model.to(target_device)\n",
            "    \n",
            "    user_input_prompt = input(\"Enter your prompt to improve: \")\n",
            "\n",
            "    if user_input_prompt:\n",
            "        print(\"\\n--- Running Prompt Improvement Chain ---\")\n",
            "        # llm_prompt_chain and its helpers should be defined in preceding cells\n",
            "        improved_prompt_interactive = llm_prompt_chain(user_input_prompt, interactive_model, tokenizer, verbose=True)\n",
            "        print(\"\\n--- End of Chain ---\")\n",
            "        print(f\"\\nOriginal User Prompt: {user_input_prompt}\")\n",
            "        print(f\"Chain's Improved Prompt: {improved_prompt_interactive}\")\n",
            "    else:\n",
            "        print(\"No prompt entered. Skipping interactive improvement.\")\n",
            "else:\n",
            "    print(\"Interactive session cannot start. Model or tokenizer not available.\")"
        ],
        "metadata": {"id": "interactive-prompt-code"},
        "execution_count": null,
        "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lUFAskYrzS_6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
